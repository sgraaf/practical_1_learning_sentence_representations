{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo notebook\n",
    "This notebook serves as a demo of my code for the InferSent practical assignment, made in the context of the SMNLS course taught at the UvA in the Spring 2019 semester. All code can also be found in [this](https://github.com/sgraaf/practical_1_learning_sentence_representations) GitHub repository.\n",
    "\n",
    "As you'll find, in the `src` directory, other than the required `train.py`, `eval.py` and `infer.py`, there are some additional files, namely:\n",
    "```\n",
    "data.py       Contains various functions for loading and preprocessing the relevant data for training\n",
    "encoders.py   Contains the four implemented encoders (Baseline, LSTM, BiLSTM and MaxBiLSTM)\n",
    "model.py      Contains the InferSent model (which uses one of the encoders in encoders.py)\n",
    "utils.py      Contains various utility functions used throughout training, evaluating, testing, etc.\n",
    "```\n",
    "\n",
    "Below, you'll find a demo of how the model predicts inference for a premise and hypothesis sentence couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Below, you'll find the imports required to run this demo. As you'll see, many of the files mentioned previously are imported and used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data import load_data\n",
    "from encoders import Baseline, BiLSTM, LSTM, MaxBiLSTM\n",
    "from model import InferSent\n",
    "from utils import load_model_state\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the data\n",
    "This cell loads the relevant data. As we're not training, we do not need the `train`, `dev`, and `test` splits of the SNLI dataset, but instead are only interested in the `text_field`, which contains our vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, text_field, _ = load_data()\n",
    "embedding = nn.Embedding.from_pretrained(text_field.vocab.vectors)\n",
    "embedding.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the encoder and model\n",
    "Below, you'll find the initialization of the encoder of choice and the model. As mentioned previously, the encoder is passed as an argument to the model, such that during training (or evaluation, etc), only the model is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InferSent(\n",
       "  (embedding): Embedding(37241, 300)\n",
       "  (encoder): Baseline()\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Baseline()\n",
    "# encoder = LSTM()\n",
    "# encoder = BiLSTM()\n",
    "# encoder = MaxBiLSTM()\n",
    "\n",
    "model = InferSent(\n",
    "    input_dim=4*encoder.output_dim,\n",
    "    hidden_dim=512,\n",
    "    output_dim=3,\n",
    "    embedding=embedding,\n",
    "    encoder=encoder\n",
    ")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model of this type\n",
    "Below, you'll find that we load the best model that belongs to this encoder type. If all went well during training, there will be no hiccups here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model state... Done!\n"
     ]
    }
   ],
   "source": [
    "models_path = Path.cwd().parent / 'output' / 'models'\n",
    "models = list(models_path.glob(f'InferSent_{encoder.__class__.__name__}_model.pt'))\n",
    "if len(models) > 0:\n",
    "    load_model_state(models[0], model)\n",
    "else:\n",
    "    raise ValueError(f'No models with the {encoder_type} encoder exist in the models directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict inference\n",
    "Below you'll find a demonstration of how this model predicts the (type of) inference between a premise sentence and a hypothesis sentence (after these have been pre-procesed to fit the model architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contradiction\n"
     ]
    }
   ],
   "source": [
    "premise = 'Bob is in his room, but because of the thunder and lightning outside, he cannot sleep'\n",
    "hypothesis = 'Bob is awake'\n",
    "\n",
    "premise = word_tokenize(premise)\n",
    "hypothesis = word_tokenize(hypothesis)\n",
    "\n",
    "premise = torch.tensor([text_field.vocab.stoi[token] for token in premise]).to(DEVICE)\n",
    "hypothesis = torch.tensor([text_field.vocab.stoi[token] for token in hypothesis]).to(DEVICE)\n",
    "\n",
    "# predict entailment\n",
    "y_pred = model.forward(\n",
    "    (premise.expand(1, -1).transpose(0, 1), torch.tensor(len(premise)).to(DEVICE)),\n",
    "    (hypothesis.expand(1, -1).transpose(0, 1), torch.tensor(len(hypothesis)).to(DEVICE))\n",
    ")\n",
    "\n",
    "# determine the type of inference\n",
    "if y_pred.argmax().item() == 0:\n",
    "    print('Entailment')\n",
    "elif y_pred.argmax().item() == 1:\n",
    "    print('Contradiction')\n",
    "elif y_pred.argmax().item() == 2:\n",
    "    print('Neutral')\n",
    "else:\n",
    "    raise ValueError('Invalid class!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
